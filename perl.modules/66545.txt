Newsgroups: perl.modules
Path: nntp.perl.org
Xref: nntp.perl.org perl.modules:66545
Return-Path: <nobody@pause.perl.org>
Mailing-List: contact modules-help@perl.org; run by ezmlm
Delivered-To: mailing list modules@perl.org
Received: (qmail 22285 invoked from network); 5 Jul 2009 08:04:05 -0000
Received: from x1.develooper.com (207.171.7.70)
  by x6.develooper.com with SMTP; 5 Jul 2009 08:04:05 -0000
Received: (qmail 21653 invoked by uid 225); 5 Jul 2009 08:04:05 -0000
Delivered-To: modules@perl.org
Received: (qmail 21649 invoked by alias); 5 Jul 2009 08:04:05 -0000
X-Spam-Status: No, hits=0.0 required=8.0
	tests=
X-Spam-Check-By: la.mx.develooper.com
Received: from pause.fiz-chemie.de (HELO pause.perl.org) (195.149.117.110)
    by la.mx.develooper.com (qpsmtpd/0.28) with ESMTP; Sun, 05 Jul 2009 01:04:00 -0700
Received: from pause.perl.org (localhost.localdomain [127.0.0.1])
	by pause.perl.org (8.13.8/8.13.8/Debian-3) with ESMTP id n6583ktt004833;
	Sun, 5 Jul 2009 10:03:46 +0200
Received: (from nobody@localhost)
	by pause.perl.org (8.13.8/8.13.8/Submit) id n6583kR3004832;
	Sun, 5 Jul 2009 10:03:46 +0200
Date: Sun, 5 Jul 2009 10:03:46 +0200
Message-ID: <200907050803.n6583kR3004832@pause.perl.org>
Subject: Module submission WWW::Spider
Reply-To: modules@perl.org
To: modules@perl.org, bytbox@cpan.org
Approved: news@nntp.perl.org
From: upload@pause.perl.org ("Perl Authors Upload Server")


The following module was proposed for inclusion in the Module List:

  modid:       WWW::Spider
  DSLIP:       cdpOp
  description: spider for fetching and analyzing websites
  userid:      BYTBOX (Scott Lawrence)
  chapterid:   15 (World_Wide_Web_HTML_HTTP_CGI)
  communities:
    comp.lang.perl.modules (has been), #perl (will be)

  similar:
    WWW::Robot, WWW::Spyder

  rationale:

    WWW::Spider is intended to be used for traversing, downloading, and
    analyzing relations between pages and sites, not for parsing HTML or
    doing general 'Network' jobs, hence the choice of the 'WWW'
    namespace. The main goal of WWW::Spider is to create a virtual graph
    (or web, hence the name 'Spider') of websites or pages, which can
    then be traversed and analyzed. For example, a simple program could
    find the link-distance between two pages on wikipedia.

    The two main related modules are WWW::Spyder and WWW::Robot (there
    is also WWW::poacher and a few others). WWW::Spyder is built on top
    of WWW::Robot, and the functionality is essentially the same
    (WWW::Spyder has slightly narrower and more detailed capabilities).
    Both are task-oriented in that their goal is to get to pages.
    WWW::Spider, on the other hand, is data-structure oriented. As a
    result, the API is drastically different, even if some of the code
    is the same. (I considered basing WWW::Spider on WWW::Robot, but
    decided against it.)

    A more complicated example program, taking use of features in
    WWW::Spider but not in the WWW::Robot-based modules, would find the
    relationship between two pages in wikipedia based, essentially, on
    the probable traffic flow from one to the other. (This is similar to
    PageRank.)

  enteredby:   BYTBOX (Scott Lawrence)
  enteredon:   Sun Jul  5 08:03:45 2009 GMT

The resulting entry would be:

WWW::
::Spider          cdpOp spider for fetching and analyzing websites   BYTBOX


Thanks for registering,
-- 
The PAUSE

PS: The following links are only valid for module list maintainers:

Registration form with editing capabilities:
  https://pause.perl.org/pause/authenquery?ACTION=add_mod&USERID=05000000_c7230bb4e2db6de5&SUBMIT_pause99_add_mod_preview=1
Immediate (one click) registration:
  https://pause.perl.org/pause/authenquery?ACTION=add_mod&USERID=05000000_c7230bb4e2db6de5&SUBMIT_pause99_add_mod_insertit=1
