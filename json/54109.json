{
   "data" : {
      "homepage" : "",
      "userid" : "LAYE",
      "fullname" : "孙磊",
      "why" : "Hi~ I'm a graduate student on NLP (natrual language processing),\n    with 2 years more Perl programming experience.\n\n    I'm planning to contribute some NLP tools to CPAN.\n\n    I'd like to start with Lingua::EN::Tokenizer which is a tokenizer\n    for English language work as:\n\n    INPUT: I've been in C.S. department. OUTPUT:\n    I/'ve/been/in/C.S./department/.\n\n    If simply split the text by spaces and punctuations, we may get the\n    wrong results. That's why a tokenizer is necessary.\n\n    The tokenizer does the different thing comparing to\n    Lingua::EN::Splitter, which just pick out words, ignoring other\n    tokens.\n\n    And, after the tokenizer, I'll proceed on a POS-tagger using a Max\n    Entropy model or CRF model. I see there's a POS-tagger in CPAN\n    Lingua::EN::Tagger, but it's based on dictionary lookup and HMM\n    model, which may fall behind in precision to Max Entropy and CRF due\n    to recent research.\n\n    Thanks\n\n    Laye",
      "mail" : "CENSORED"
   },
   "meta" : {
      "message_id" : "200704221206.l3MC6VqE019767@pause.perl.org",
      "epoch" : 1177243591,
      "from" : "upload@pause.perl.org",
      "subject" : "PAUSE ID request (LAYE;=?UTF-8?Q?=20=E5=AD=99=E7=A3=8A?=)",
      "file" : "54109.txt",
      "type" : "pause_id_request"
   }
}
