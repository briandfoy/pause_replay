{
   "data" : {
      "userid" : {
         "name" : "Nicholas C. Baker",
         "id" : "NICKBAKER"
      },
      "description" : "Automates archiving of entire web sites",
      "communities" : "",
      "chapterid" : {
         "name" : "World_Wide_Web_HTML_HTTP_CGI",
         "id" : "15"
      },
      "similar" : "HTTP::GetImages",
      "DSLIP" : "bdpfp",
      "modid" : "HTTP::ArchiveSite",
      "enteredon" : "Fri Aug  6 20:12:42 2004 GMT",
      "enteredby" : {
         "name" : "Nicholas C. Baker",
         "id" : "NICKBAKER"
      },
      "PS" : "The following links are only valid for module list maintainers:",
      "rationale" : "I named it HTTP::ArchiveSite because it archives websites. The\n    uniqueness of my approach is that it modifies the downloaded files\n    so that they are browsable locally, by making links to other\n    spidered files relative, and links to non-spidered files absolute.\n    Without the disclaimer at the top of each page, you'd never know you\n    were browsing offline. At least that's the idea, it still has a few\n    kinks to be worked out.\n\n    The whole project was inspired by my work for the Internet Public\n    Library (www.ipl.org) which links to many outside URLs and suffers\n    from the pain of Link Rot. With this program they could provide\n    their users with cached copies of the sites they link to."
   },
   "meta" : {
      "message_id" : "200408062012.i76KCjHN003502@pause.perl.org",
      "epoch" : 1091823165,
      "from" : "upload@pause.perl.org",
      "subject" : "Module submission HTTP::ArchiveSite",
      "file" : "34020.txt",
      "type" : "module_submission"
   }
}
